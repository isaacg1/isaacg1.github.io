---
published: true
title: 'Deanthropomorphizing AI & Long-term Safety'
layout: post
---

I am interested in studying the long-term safety of AI systems.
However, I think the terminology that is currently in use in this area
inhibits our understanding.
Current terminology is often anthropomorphic (based on human analogies)
or fanciful/hyperbolic (based on science-fiction analogies).
I think anthropomorphic language biases our thinking towards modeling
these systems like humans,
which can blind to what's going on.
I think fanciful and hyperbolic language biases our thinking towards
something we might have seen in a story,
which can lead us to not take things seriously,
or to use narrative-style reasoning.

Therefore, I'm going to write out a glossary giving a new set of terms
for important concepts in this area,
which will hopefully make it easier to think about these questions.
I'm mostly suggesting this as an aid to one's own thinking,
or for small-group communication.

For each concept, I'll give my own term,
as well as the standard analogous term,
and a definition using deanthropomophized language.
By the end of this piece,
I'll discuss my thoughts about long-term safety
using this new terminology.

## Glossary

### Previously-mental automation (PMA)

Standard term: "Artificial Intelligence".
I think that the word "Intelligence" is overly anthropomorphic -- it encourages us to 
think about an automated system as if it is a human or other animal.
Because the mechanisms by which these automated systems operate is sufficiently different
from how brains operate, I do not think this analogy is useful.

Instead, I suggest the term "Previously-mental automation" (PMA).
This term should be thought of as a noun: "This program is a PMA."
By "Previously-mental", I mean that the automated system is doing something that,
prior to the creation of such an automated system, we previously associated
with human mental skills.
This puts the focus on the nature of the task being performed,
not the mechanism by which the task is accomplished.

Examples of PMAs:

* Chess-playing programs. Playing chess well is considered, in humans,
a challenging mental skill.

* Machine translation. Translation of speech or text
from one language to another is considered, in humans,
a challenging mental skill and an important profession.

* Autopilots in planes.

* Optical character recognition.

### General-area PMA (GAPMA)

Standard term: "Artificial General Intelligence".
Again, the "Intelligence" framing puts the focus on how the task is performed,
rather than what task is accomplished,
making it unclear whether "General" refers to a kind of task or a kind of intelligence.
Again, I think this is overly anthropomorphic and not helpful.

Instead, "General-area PMA" (GAPMA) puts the focus on the set of tasks being performed.
A GAPMA is an automated system that is capable of a wide range of tasks
that were previously associated with human mental skills.
This should be thought of as a sliding scale.
At one extreme,
a very narrow-area PMA would be one that can only do
a single previously-mental task well, such as a chess bot
or a translation program between two specific languages.
Somewhat more general would be an automated system
that can do a set of related previously-mental tasks.
Examples would include a program that can perform well at playing
a wide variety of board games,
or a program that can translate between a wide variety of languages.

Even more general would be an automated system that
can do essentially unrelated tasks, or tasks that were not previously thought of as related.
The most prominent recent example is the GPT family of text prediction programs,
which can (with appropriate prompting)
do tasks such as translation, essay generation, programming, etc.
While the quality of performance of such tasks is variable,
the set of topics is highly general.

### Uncontrolled PMA

### Feedback PMA

### Uncontrolled Feedback GAPMA 
